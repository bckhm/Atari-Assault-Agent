{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bckhm/Blackjack-Agent/blob/main/Blackjack_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5",
      "metadata": {
        "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5"
      },
      "source": [
        "# Blackjack Agent using Deep-Q Learning\n",
        "In this notebook, we aim to train an agent to play the Blackjack via the use of reinforcement learning (Deep-Q Learning). We will treat the process of walking as a Markov Decision Process (MDP) in our learning efforts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Github/Blackjack-Agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC84zDpqZ_CO",
        "outputId": "890128d1-8a2d-464f-e99b-e099ad4844dc"
      },
      "id": "GC84zDpqZ_CO",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/Github/Blackjack-Agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "401d8655-4b98-41b6-8043-239cf300d997",
      "metadata": {
        "id": "401d8655-4b98-41b6-8043-239cf300d997"
      },
      "source": [
        "# 1. Packages\n",
        "`Xvfb` and `gym[Box2D]` are also required in order to run our display and the environment respectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxfoOJl-adp8",
        "outputId": "1e7af7ce-ee81-4b7f-e182-b4fb8b85b1c7"
      },
      "id": "BxfoOJl-adp8",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.8/dist-packages (3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHZN9WWMazj_",
        "outputId": "9401c5fe-4b72-4ced-bfd5-f91c90bbc0ac"
      },
      "id": "pHZN9WWMazj_",
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65",
      "metadata": {
        "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "3d331466-0731-4931-9334-138a2110c485",
      "metadata": {
        "id": "3d331466-0731-4931-9334-138a2110c485"
      },
      "outputs": [],
      "source": [
        "# Virtual Display for rendering of the Bipedal Walker environment\n",
        "Display(visible=0, size=(840, 480)).start();\n",
        "\n",
        "# Random seed for tensorflow\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Blackjack Environment\n",
        "We will [OpenAI's Gym library](https://www.gymlibrary.dev/) to load and attempt to solve the Blackjack environment. \n",
        "\n",
        "The goal of the Blakcjack environment is to train an agent to beat the dealer in Blackjack by obtaining cards that sum close to 21, without going over 21, and yet still have a higher value thant the dealer's card.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<figure>\n",
        "  <img src =\"https://www.gymlibrary.dev/_images/blackjack.gif\" width = 40%>\n",
        "      <figcaption style = \"text-align: center; font-style: italic\">Atari Environment</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "KQQxtBtSFAl1"
      },
      "id": "KQQxtBtSFAl1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Action Space\n",
        "The action space consists of two actions represented by discrete values.\n",
        "- `0`: Stick\n",
        "- `1`: Hit"
      ],
      "metadata": {
        "id": "dKGIwjAcHBmh"
      },
      "id": "dKGIwjAcHBmh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Observation Space\n",
        "The agent's observation space is a state vector containing 3 variables:\n",
        "1. Player's current sum\n",
        "2. Dealer's one showing card (1- 10)\n",
        "3. Whether a player holds a usable ace\n"
      ],
      "metadata": {
        "id": "VGjFG3boHb4K"
      },
      "id": "VGjFG3boHb4K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Rewards\n",
        "- Win game: +1\n",
        "- Lose game: -1\n",
        "- Draw: 0\n",
        "- Win game with natural Blackjack: +1.5 if `natural=True`, else +1"
      ],
      "metadata": {
        "id": "Qg34xGn_IwBu"
      },
      "id": "Qg34xGn_IwBu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loading the Environement\n",
        "We use the `gym` library to open the `BipedalWalker-v3` environment. `.reset()` resets the environment and `.render()` renders the first frame of the environment."
      ],
      "metadata": {
        "id": "sejlakzRMp-K"
      },
      "id": "sejlakzRMp-K"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v1', natural=True, sab=False)\n",
        "env.reset()\n",
        "\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "id": "bH3Bw4upNOn5",
        "outputId": "b30fbbfd-f6e5-4c82-8848-98882a4a1991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "id": "bH3Bw4upNOn5",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x500 at 0x7F74AAA103D0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thereafter, we check the size of the state vector as well as the number of possible actions "
      ],
      "metadata": {
        "id": "UjSfxNuPNfvn"
      },
      "id": "UjSfxNuPNfvn"
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape = len(env.observation_space)\n",
        "poss_actions_space = env.action_space\n",
        "\n",
        "print(f\"State Vector: {state_shape}\")\n",
        "print(f\"Poss Actions: {poss_actions}\")"
      ],
      "metadata": {
        "id": "eSPw4v_EO2jK",
        "outputId": "e997d7b2-6afc-44e4-c221-82b05d121216",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eSPw4v_EO2jK",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Vector: 3\n",
            "Poss Actions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poss_actions = 2"
      ],
      "metadata": {
        "id": "Z5yuwRfNdf7W"
      },
      "id": "Z5yuwRfNdf7W",
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Dynamics of the Environment\n",
        "Running `.step(action)` on the environment runs a single time step from the current state, $s$, taking action $a$ and returning 4 different values:\n",
        "\n",
        "1. `next_state` $(object)$: Observation of the environment after an action. This is a vector contain our Observation Space variables. We may sometimes denote this as $s'$\n",
        "\n",
        "2. `rewards` $(float)$: Reward received as a result of our action. We may sometimes denote this as $R$\n",
        "\n",
        "3. `done` $(bool)$: Indiciating whether an episode has terminated\n",
        "\n",
        "4. `info` $(dictionary)$: Diagnostics used for debugging."
      ],
      "metadata": {
        "id": "E3gkbnmnQasz"
      },
      "id": "E3gkbnmnQasz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial state of environment\n",
        "init_state = env.reset()\n",
        "\n",
        "# Sample action\n",
        "action = 0\n",
        "\n",
        "# Run a single time step given the action\n",
        "next_state, reward, done, _ = env.step(action) "
      ],
      "metadata": {
        "id": "9fcdAgTcRpvv"
      },
      "id": "9fcdAgTcRpvv",
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_table(init_state, action, next_state, reward, done)"
      ],
      "metadata": {
        "id": "Wzf8UwFSUXAu",
        "outputId": "15d08130-0b00-4b1b-d2b5-fc6e6de5ca49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "id": "Wzf8UwFSUXAu",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-37b97c1e400e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Github/Atari-Assault-Agent/utils.py\u001b[0m in \u001b[0;36mdisplay_table\u001b[0;34m(initial_state, action, next_state, reward, done)\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Deep $Q$-Learning\n",
        "## 5.1 $Q$ Function\n",
        "Our $Q$ function takes in 2 inputs -- $s$ and $a$, the current state and the action taken from this state. $Q(s, a)$ in this case is the total return for taking action $a$, arriving at state $s'$, and performing optimal actions from then on. We formulate $Q(s, a)$ using the Bellman Equation as shown:\n",
        "\n",
        "$$Q_{i+1}(s, a) = R + \\gamma\\ max_{a'}\\ Q_{i}(s', a')$$\n",
        "\n",
        "Whereby $\\gamma$ is the discount factor. As $i \\to \\infty$, our $Q$ function converges to the optimal $Q^{*}$. Due to the continuous problem that we are facing, we are unable to explore the entire state-action space and instead make use of a neural network, denoted as the $Q$-network, to estimate $Q(s, a) \\approx Q^{*}(s, a)$ via iteratively adjusting its weights using gradient descent.\n",
        "\n",
        "The $Q^{*}$ function is then used to choose the action that maximises $Q^{*}(s, a)$ to gain the greatest reward\n",
        "\n",
        "## 5.2 Target Network\n",
        "Our error term is currently calculated as shown\n",
        "$$\n",
        "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
        "$$\n",
        "\n",
        "However, the constantly changing $y$ targets may lead to instabilities in this application of reinforcement learning. Therefore, we need a seperate NN to be calculating the $y$ targets, whose weights are updated at a significantly slower pace. We denote the target network's state-action function as $\\hat{Q}$ and its weights as $w^{-}$. We update $w^{-}$  using a **soft update** as follows:\n",
        "\n",
        "$$w^-\\leftarrow \\tau w + (1 - \\tau) w^-$$\n",
        "\n",
        "whereby $\\tau < 1$. And our error is calculated as shown:\n",
        "\n",
        "$$ \\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}} $$\n",
        "\n",
        "## 5.3 Networks\n",
        "Here, the networks for both the target network as well as the $Q$-network are built using `keras`. Our initial architecture is as follows:\n",
        "1. `Input` layer: takes in `state_shape` as inpit\n",
        "2. `Dense` layer: `64` units, `relu` activation\n",
        "3. `Dense` layer: `64` units, `relu` activation\n",
        "4. `Dense` layer: `poss_actions` units, `linear` activation\n",
        "\n",
        "We use the `Adam` optimizer here, and also initate our intiial parameters."
      ],
      "metadata": {
        "id": "XwEMud4LUdT_"
      },
      "id": "XwEMud4LUdT_"
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMETERS\n",
        "ALPHA = 1e-3 # Learning rate\n",
        "GAMMA = 0.8 # Discount factor\n",
        "MEM_SIZE = 100000 # Memory buffer size\n",
        "NUM_STEPS_FOR_UPDATE = 4 # Number of time steps before updating weights"
      ],
      "metadata": {
        "id": "1_6BHxFJsF0c"
      },
      "id": "1_6BHxFJsF0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-network\n",
        "q_network = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(state_shape)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(poss_actions[0], activation='linear')\n",
        "])\n",
        "\n",
        "# Target Network\n",
        "target_network = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(state_shape)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(poss_actions[0], activation='linear')\n",
        "])\n",
        "\n",
        "# Optimiser\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=ALPHA)"
      ],
      "metadata": {
        "id": "0kuCCJ5xtsb2"
      },
      "id": "0kuCCJ5xtsb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Experience Relay\n",
        "**Experience Relay** is used to prevent consecutive time steps being used due to their higher correlation. This process consists of storing all our experiences, which are in the form $(s, a, R, s')$ in a **memory buffer**, which will be randomly sampled form in **mini-batches** that reduce runtime. We store each individual experienced as a `namedtuple`.\n",
        "\n",
        "Experience relay will be applied later on while training the agent."
      ],
      "metadata": {
        "id": "Wrl1lfWpx07g"
      },
      "id": "Wrl1lfWpx07g"
    },
    {
      "cell_type": "code",
      "source": [
        "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\" ])"
      ],
      "metadata": {
        "id": "nsJ81AQx3_6A"
      },
      "id": "nsJ81AQx3_6A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Loss Function\n",
        "We compute the loss using Mean Squared Error, and the error term will follow section 5.1 whereby $y$-target is obtained from the Target Network. One thing to note is that $y$ no longer follows the Bellman equation if the next step is at a **terminal state**.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    y_j =\n",
        "    \\begin{cases}\n",
        "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
        "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
        "    \\end{cases}       \n",
        "\\end{equation}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "e8IEaCcW4Ite"
      },
      "id": "e8IEaCcW4Ite"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
        "    \"\"\" \n",
        "    Calculates the loss.\n",
        "    \n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
        "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
        "          \n",
        "    Returns:\n",
        "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
        "            the y targets and the Q(s,a) values.\n",
        "    \"\"\"\n",
        "    # Unpack mini-batch\n",
        "    states, actions, rewards, next_states, done_vals = experiences\n",
        "\n",
        "    # Find Max Q^(s, a) using the Target Network's Q^ function\n",
        "    max_qsa = tf.reduce_max(target_network(next_states), axis=-1)\n",
        "\n",
        "    # Compute y-target\n",
        "    y_targets = rewards + (1 - done_vals) * gamma * max_qsa\n",
        "\n",
        "    # Get Q(s,a) values from the Q-network\n",
        "    q_values = q_network(states)\n",
        "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
        "                                               tf.cast(actions, tf.int32)], axis=1))\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = tf.keras.losses.MSE(y_targets, q_values)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRo7vyj45Iao"
      },
      "id": "qRo7vyj45Iao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Function to Update Network Weights\n",
        "A custom training loop is employed in order to update both the weights of the $Q$ and $\\hat{Q}$. A soft-update as mentioned in 5.2 is used for $\\hat{Q}$"
      ],
      "metadata": {
        "id": "Z6UITp4v6E1H"
      },
      "id": "Z6UITp4v6E1H"
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def agent_learn(experiences, gamma):\n",
        "    \"\"\"\n",
        "    Updates the weights of the Q networks.\n",
        "    \n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate the loss.\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(experiences, gamma, q_network, target_network)\n",
        "\n",
        "    # Get the gradients of the loss with respect to the weights.\n",
        "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "    \n",
        "    # Update the weights of the q_network.\n",
        "    optimiser.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "\n",
        "    # update the weights of target network via (soft update.\n",
        "    # Function from utils\n",
        "\n",
        "    TAU = 1e-3\n",
        "    update_target_network(q_network, target_network, TAU)"
      ],
      "metadata": {
        "id": "4bahrEU-6_Ci"
      },
      "id": "4bahrEU-6_Ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Agent Training"
      ],
      "metadata": {
        "id": "oZzBFTcn7D9n"
      },
      "id": "oZzBFTcn7D9n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep track of learning time\n",
        "start = time.time()\n",
        "\n",
        "# Time requirement for the env\n",
        "max_num_timesteps = 1600\n",
        "NUM_EPISODES = 2000 # Our rough estimate of the number of episodes we need\n",
        "\n",
        "# Requirement for how many trials to average\n",
        "num_p_av = 100 \n",
        "\n",
        "# Initial Epsilon for epsilon greedy policy\n",
        "epsilon = 1.0\n",
        "\n",
        "# Keeping track of point history\n",
        "total_point_history = []\n",
        "\n",
        "# Memory buffer, containing experience namedtuples\n",
        "memory_buffer = deque(maxlen=MEMORY_SIZE)"
      ],
      "metadata": {
        "id": "aHCMdveZi3qv"
      },
      "id": "aHCMdveZi3qv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial target weights using q_network's weights\n",
        "target_network.set_weights(q_network.get_weights())\n",
        "\n",
        "# Iterate NUM_EPISODES times\n",
        "for i in range(NUM_EPISODES):\n",
        "  # Rest env and start from the initial state\n",
        "  state = env.reset()\n",
        "  total_points = 0\n",
        "\n",
        "  # Iterate max number of time steps\n",
        "  for t in range(max_num_timesteps):\n",
        "\n",
        "    # Choose action a using current state\n",
        "    state_qn = np.expand_dims(state, axis=0) # Reshape to fit the q network\n",
        "    q_values = q_network(state_qn) # Get Q(s, a) values given current state\n",
        "    action = get_action(q_values, epsilon) # In Utils, use epsilon greedy policy \n"
      ],
      "metadata": {
        "id": "9zM3z_lzj8xk"
      },
      "id": "9zM3z_lzj8xk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwNgRmstsyK7"
      },
      "id": "OwNgRmstsyK7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}