{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5",
   "metadata": {
    "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5"
   },
   "source": [
    "# Blackjack Agent using Deep-Q Learning\n",
    "In this notebook, we aim to train an agent to play the Blackjack via the use of reinforcement learning (Deep-Q Learning). We will treat the process of walking as a Markov Decision Process (MDP) in our learning efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d8655-4b98-41b6-8043-239cf300d997",
   "metadata": {
    "id": "401d8655-4b98-41b6-8043-239cf300d997"
   },
   "source": [
    "# 1. Packages\n",
    "`Xvfb` and `gym[Box2D]` are also required in order to run our display and the environment respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65",
   "metadata": {
    "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from utils import *\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d331466-0731-4931-9334-138a2110c485",
   "metadata": {
    "id": "3d331466-0731-4931-9334-138a2110c485"
   },
   "outputs": [],
   "source": [
    "# Virtual Display for rendering of the Bipedal Walker environment\n",
    "Display(visible=0, size=(840, 480)).start();\n",
    "\n",
    "# Random seed for tensorflow\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KQQxtBtSFAl1",
   "metadata": {
    "id": "KQQxtBtSFAl1"
   },
   "source": [
    "# 2. Blackjack Environment\n",
    "We will [OpenAI's Gym library](https://www.gymlibrary.dev/) to load and attempt to solve the Blackjack environment. \n",
    "\n",
    "The goal of the Blakcjack environment is to train an agent to beat the dealer in Blackjack by obtaining cards that sum close to 21, without going over 21, and yet still have a higher value thant the dealer's card.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<figure>\n",
    "  <img src =\"https://www.gymlibrary.dev/_images/blackjack.gif\" width = 40%>\n",
    "      <figcaption style = \"text-align: center; font-style: italic\">Atari Environment</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dKGIwjAcHBmh",
   "metadata": {
    "id": "dKGIwjAcHBmh"
   },
   "source": [
    "## 2.1 Action Space\n",
    "The action space consists of two actions represented by discrete values.\n",
    "- `0`: Stick\n",
    "- `1`: Hit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VGjFG3boHb4K",
   "metadata": {
    "id": "VGjFG3boHb4K"
   },
   "source": [
    "## 2.2 Observation Space\n",
    "The agent's observation space is a state vector containing 3 variables:\n",
    "1. Player's current sum `[int]`\n",
    "2. Dealer's one showing card (1- 10) `[int]`\n",
    "3. Whether a player holds a usable ace `[bool]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qg34xGn_IwBu",
   "metadata": {
    "id": "Qg34xGn_IwBu"
   },
   "source": [
    "## 2.3 Rewards\n",
    "- Win game: +1\n",
    "- Lose game: -1\n",
    "- Draw: 0\n",
    "- Win game with natural Blackjack: +1.5 if `natural=True`, else +1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sejlakzRMp-K",
   "metadata": {
    "id": "sejlakzRMp-K"
   },
   "source": [
    "# 3. Loading the Environement\n",
    "We use the `gym` library to open the `BipedalWalker-v3` environment. `.reset()` resets the environment and `.render()` renders the first frame of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bH3Bw4upNOn5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 656
    },
    "id": "bH3Bw4upNOn5",
    "outputId": "b30fbbfd-f6e5-4c82-8848-98882a4a1991"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=600x500 at 0x290F99060>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v1', natural=True, sab=False, render_mode='rgb_array')\n",
    "env.reset()\n",
    "\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UjSfxNuPNfvn",
   "metadata": {
    "id": "UjSfxNuPNfvn"
   },
   "source": [
    "Thereafter, we check the size of the state vector as well as the number of possible actions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eSPw4v_EO2jK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSPw4v_EO2jK",
    "outputId": "e997d7b2-6afc-44e4-c221-82b05d121216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Vector: 3\n",
      "Poss Actions: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "state_shape = len(env.observation_space)\n",
    "poss_actions_space = env.action_space\n",
    "\n",
    "print(f\"State Vector: {state_shape}\")\n",
    "print(f\"Poss Actions: {poss_actions_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "Z5yuwRfNdf7W",
   "metadata": {
    "id": "Z5yuwRfNdf7W"
   },
   "outputs": [],
   "source": [
    "poss_actions = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E3gkbnmnQasz",
   "metadata": {
    "id": "E3gkbnmnQasz"
   },
   "source": [
    "# 4. Dynamics of the Environment\n",
    "Running `.step(action)` on the environment runs a single time step from the current state, $s$, taking action $a$ and returning different values. Here, we only care about:\n",
    "\n",
    "1. `next_state` $(object)$: Observation of the environment after an action. This is a vector contain our Observation Space variables. We may sometimes denote this as $s'$\n",
    "\n",
    "2. `rewards` $(float)$: Reward received as a result of our action. We may sometimes denote this as $R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fcdAgTcRpvv",
   "metadata": {
    "id": "9fcdAgTcRpvv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 10, False), -1.0, True, False, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial state of environment\n",
    "init_state = env.reset()\n",
    "\n",
    "# Sample action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step given the action\n",
    "next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b44cee-7e61-4b06-b54d-514dad95be01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 10, False), {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Wzf8UwFSUXAu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "Wzf8UwFSUXAu",
    "outputId": "15d08130-0b00-4b1b-d2b5-fc6e6de5ca49"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Initial State:</th>      <td>((10, 10, False), {})</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Action:</th>                       <td>S</td>          \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Next State:</th>            <td>(10, 10, False)</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Reward Received:</th>           <td>-1.000</td>        \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Episode Terminated:</th>         <td>True</td>         \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_table(init_state, action, next_state, reward, done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XwEMud4LUdT_",
   "metadata": {
    "id": "XwEMud4LUdT_"
   },
   "source": [
    "# 5. Deep $Q$-Learning\n",
    "## 5.1 $Q$ Function\n",
    "Our $Q$ function takes in 2 inputs -- $s$ and $a$, the current state and the action taken from this state. $Q(s, a)$ in this case is the total return for taking action $a$, arriving at state $s'$, and performing optimal actions from then on. We formulate $Q(s, a)$ using the Bellman Equation as shown:\n",
    "\n",
    "$$Q_{i+1}(s, a) = R + \\gamma\\ max_{a'}\\ Q_{i}(s', a')$$\n",
    "\n",
    "Whereby $\\gamma$ is the discount factor. As $i \\to \\infty$, our $Q$ function converges to the optimal $Q^{*}$. Due to the continuous problem that we are facing, we are unable to explore the entire state-action space and instead make use of a neural network, denoted as the $Q$-network, to estimate $Q(s, a) \\approx Q^{*}(s, a)$ via iteratively adjusting its weights using gradient descent.\n",
    "\n",
    "The $Q^{*}$ function is then used to choose the action that maximises $Q^{*}(s, a)$ to gain the greatest reward\n",
    "\n",
    "## 5.2 Target Network\n",
    "Our error term is currently calculated as shown\n",
    "$$\n",
    "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
    "$$\n",
    "\n",
    "However, the constantly changing $y$ targets may lead to instabilities in this application of reinforcement learning. Therefore, we need a seperate NN to be calculating the $y$ targets, whose weights are updated at a significantly slower pace. We denote the target network's state-action function as $\\hat{Q}$ and its weights as $w^{-}$. We update $w^{-}$  using a **soft update** as follows:\n",
    "\n",
    "$$w^-\\leftarrow \\tau w + (1 - \\tau) w^-$$\n",
    "\n",
    "whereby $\\tau < 1$. And our error is calculated as shown:\n",
    "\n",
    "$$ \\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}} $$\n",
    "\n",
    "## 5.3 Networks\n",
    "Here, the networks for both the target network as well as the $Q$-network are built using `keras`. Our initial architecture is as follows:\n",
    "1. `Input` layer: takes in `state_shape` as inpit\n",
    "2. `Dense` layer: `8` units, `relu` activation\n",
    "3. `Dense` layer: `8` units, `relu` activation\n",
    "4. `Dense` layer: `poss_actions` units, `linear` activation\n",
    "\n",
    "Furthermore, since each blackjack episode only lasts 1 - 3 states, we want to penalise more risky plays by having a lower discount factor $\\gamma$.\n",
    "\n",
    "We use the `Adam` optimizer here, and also initate our intiial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1_6BHxFJsF0c",
   "metadata": {
    "id": "1_6BHxFJsF0c"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "ALPHA = 1e-3 # Learning rate\n",
    "GAMMA = 0.33 # Discount factor\n",
    "MEM_SIZE = 100000 # Memory buffer size\n",
    "NUM_STEPS_FOR_UPDATE = 1 # Number of time steps before updating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0kuCCJ5xtsb2",
   "metadata": {
    "id": "0kuCCJ5xtsb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 18:12:37.695799: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-20 18:12:37.695921: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Q-network\n",
    "q_network = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(state_shape)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(poss_actions, activation='linear')\n",
    "])\n",
    "\n",
    "# Target Network\n",
    "target_network = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(state_shape)),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(poss_actions, activation='linear')\n",
    "])\n",
    "\n",
    "# Optimiser\n",
    "optimiser = tf.keras.optimizers.Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wrl1lfWpx07g",
   "metadata": {
    "id": "Wrl1lfWpx07g"
   },
   "source": [
    "## 5.4 Experience Relay\n",
    "**Experience Relay** is used to prevent consecutive time steps being used due to their higher correlation. This process consists of storing all our experiences, which are in the form $(s, a, R, s')$ in a **memory buffer**, which will be randomly sampled form in **mini-batches** that reduce runtime. We store each individual experienced as a `namedtuple`.\n",
    "\n",
    "Experience relay will be applied later on while training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nsJ81AQx3_6A",
   "metadata": {
    "id": "nsJ81AQx3_6A"
   },
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8IEaCcW4Ite",
   "metadata": {
    "id": "e8IEaCcW4Ite"
   },
   "source": [
    "## 5.5 Loss Function\n",
    "We compute the loss using Mean Squared Error, and the error term will follow section 5.1 whereby $y$-target is obtained from the Target Network. One thing to note is that $y$ no longer follows the Bellman equation if the next step is at a **terminal state**.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    y_j =\n",
    "    \\begin{cases}\n",
    "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
    "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "qRo7vyj45Iao",
   "metadata": {
    "id": "qRo7vyj45Iao"
   },
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "    # Unpack mini-batch\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "\n",
    "    # Find Max Q^(s, a) using the Target Network's Q^ function\n",
    "    max_qsa = tf.reduce_max(target_network(next_states), axis=-1)\n",
    "\n",
    "    # Compute y-target\n",
    "    y_targets = rewards + (1 - done_vals) * gamma * max_qsa\n",
    "\n",
    "    # Get Q(s,a) values from the Q-network\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                               tf.cast(actions, tf.int32)], axis=1))\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = tf.keras.losses.MSE(y_targets, q_values)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z6UITp4v6E1H",
   "metadata": {
    "id": "Z6UITp4v6E1H"
   },
   "source": [
    "## 5.6 Function to Update Network Weights\n",
    "A custom training loop is employed in order to update both the weights of the $Q$ and $\\hat{Q}$. A soft-update as mentioned in 5.2 is used for $\\hat{Q}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bahrEU-6_Ci",
   "metadata": {
    "id": "4bahrEU-6_Ci"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss.\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimiser.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target network via (soft update.\n",
    "    # Function from utils\n",
    "\n",
    "    TAU = 1e-3\n",
    "    update_target_network(q_network, target_network, TAU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oZzBFTcn7D9n",
   "metadata": {
    "id": "oZzBFTcn7D9n"
   },
   "source": [
    "# 6. Agent Training\n",
    "With the wide application of reinforcement learning in Blackjack with poor results, we try to beat the benchmark of a 40% win rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aHCMdveZi3qv",
   "metadata": {
    "id": "aHCMdveZi3qv"
   },
   "outputs": [],
   "source": [
    "# Keep track of learning time\n",
    "start = time.time()\n",
    "\n",
    "# Time requirement for the env\n",
    "max_num_timesteps = 3\n",
    "NUM_EPISODES = 30000 # Our rough estimate of the number of episodes we need\n",
    "\n",
    "# Requirement for how many trials to average\n",
    "num_p_av = 5000\n",
    "\n",
    "# Initial Epsilon for epsilon greedy policy\n",
    "epsilon = 1.0\n",
    "\n",
    "# Keeping track of point history\n",
    "total_point_history = []\n",
    "\n",
    "# Memory buffer, containing experience namedtuples\n",
    "memory_buffer = deque(maxlen=MEM_SIZE)\n",
    "\n",
    "# Wins/losses/draws\n",
    "stats = {'wins': 0, 'losses': 0, 'draws': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9zM3z_lzj8xk",
   "metadata": {
    "id": "9zM3z_lzj8xk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 66 | Total point average of the last 5000 episodes: -0.39"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 18:12:38.425112: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-20 18:12:38.425292: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5000 | Total point average of the last 5000 episodes: -0.59\n",
      "Episode 10000 | Total point average of the last 5000 episodes: -0.54\n",
      "Episode 15000 | Total point average of the last 5000 episodes: -0.52\n",
      "Episode 20000 | Total point average of the last 5000 episodes: -0.47\n",
      "Episode 25000 | Total point average of the last 5000 episodes: -0.51\n",
      "Episode 30000 | Total point average of the last 5000 episodes: -0.47\n",
      "\n",
      "Total Runtime: 654.37 s (10.91 min)\n",
      "Win rate: 0.3775\n"
     ]
    }
   ],
   "source": [
    "# Set initial target weights using q_network's weights\n",
    "target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "# Iterate NUM_EPISODES times\n",
    "for i in range(NUM_EPISODES):\n",
    "  # Rest env and start from the initial state\n",
    "    state = env.reset()[0]\n",
    "    total_points = 0\n",
    "\n",
    "  # Iterate max number of time steps\n",
    "    for t in range(max_num_timesteps):\n",
    "\n",
    "        # Choose action a from curr state using Epsilon Greedy Policy\n",
    "        state = convert_bool(list(state))\n",
    "        state_qn = np.expand_dims(state, axis=0) # Reshape to fit the q network\n",
    "        q_values = q_network(state_qn) # Get Q(s, a) values given current state\n",
    "        action = get_action(q_values, epsilon) # In Utils, use epsilon greedy policy\n",
    "        \n",
    "        # Take action a from current state\n",
    "        next_state, reward, _, done, _ = env.step(action)\n",
    "        \n",
    "        # Store experience tupple (s, a, R, s') in memoru buffer\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # Check if we've hit the appropriate t time steps\n",
    "        # And if memory buffer has sufficient data points for minibatch\n",
    "        update = check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if reward == 1.0 or reward == 1.5:\n",
    "            stats['wins'] += 1\n",
    "        \n",
    "        elif reward == 0:\n",
    "            stats['draws'] += 1\n",
    "            \n",
    "        else:\n",
    "            stats['losses'] += 1\n",
    "        \n",
    "        if update:\n",
    "            # MINIBATCH Sampling of memory buffer.\n",
    "            # MINIBATCH_SIZE denoted in utils\n",
    "            experiences = get_experiences(memory_buffer)\n",
    "            \n",
    "            # Perform gradient descent step, and update target Network using soft-update\n",
    "            agent_learn(experiences, GAMMA)\n",
    "            \n",
    "        state = deepcopy(next_state)\n",
    "        total_points += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    total_point_history.append(total_points)\n",
    "    mean_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "            \n",
    "    # Update episilon value for e-greedy policy (utils)\n",
    "    episilon = get_new_eps(epsilon)\n",
    "            \n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {mean_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {mean_latest_points:.2f}\")\n",
    "\n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 0.5 points in the last 100 episodes.\n",
    "    if mean_latest_points >= 0.4:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('blackjack_agent_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")\n",
    "print(f\"Win rate: {stats['wins'] / (sum(stats.values()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f72d39-9b44-4a6e-ad4d-e1261641351e",
   "metadata": {},
   "source": [
    "# 7. Conclusion\n",
    "For a simple probablistic game such as Blackjack, it might be more relevant to use a traditional policy or a statistical model rather than applying Deep $Q$-learning. As seen from our results of a constant negative average reward and ultimately negative win-rate, applying neural networks to infer estimate the $Q*$ function doesn't work too well, and our results oscilates."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
