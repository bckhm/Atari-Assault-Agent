{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5",
      "metadata": {
        "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5"
      },
      "source": [
        "# Bipedal Walker Training using Deep-Q Learning\n",
        "In this notebook, we aim to train an agent (Walker) to walk via the use of reinforcement learning (Deep-Q Learning). We will treat the process of walking as a Markov Decision Process (MDP) in our learning efforts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Github/Bipedal-Walker-Training"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC84zDpqZ_CO",
        "outputId": "9f98f301-7d9b-4645-b54f-6713b33610a6"
      },
      "id": "GC84zDpqZ_CO",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Github/Bipedal-Walker-Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "401d8655-4b98-41b6-8043-239cf300d997",
      "metadata": {
        "id": "401d8655-4b98-41b6-8043-239cf300d997"
      },
      "source": [
        "# 1. Packages\n",
        "`Xvfb` and `gym[Box2D]` are also required in order to run our display and the environment respectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxfoOJl-adp8",
        "outputId": "3ea057b9-6730-4b31-c75b-4cfc578fce55"
      },
      "id": "BxfoOJl-adp8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHZN9WWMazj_",
        "outputId": "3f42e68e-a1e5-43d3-b81e-12b856f7cfc9"
      },
      "id": "pHZN9WWMazj_",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n",
            "Collecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.11.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65",
      "metadata": {
        "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3d331466-0731-4931-9334-138a2110c485",
      "metadata": {
        "id": "3d331466-0731-4931-9334-138a2110c485"
      },
      "outputs": [],
      "source": [
        "# Virtual Display for rendering of the Bipedal Walker environment\n",
        "Display(visible=0, size=(840, 480)).start();\n",
        "\n",
        "# Random seed for tensorflow\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Bipedal Walker Environment\n",
        "We will [OpenAI's Gym library](https://www.gymlibrary.dev/) to load and attempt to solve the Bipedal Walker environment. \n",
        "\n",
        "The goal of the Bipedal Walker environment is to train a 4-joint walker to walk on slightly uneven terrian. We attempt to solve the normal version here, and in order to do so we need to achieve an average of `300` points over 100 coonsecutive trials in `1600` time steps.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<figure>\n",
        "  <img src =\"https://www.gymlibrary.dev/_images/bipedal_walker.gif\" width = 40%>\n",
        "      <figcaption style = \"text-align: center; font-style: italic\">Bipedal Walker Environment</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "KQQxtBtSFAl1"
      },
      "id": "KQQxtBtSFAl1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Action Space\n",
        "The action space consists of motor speed values in the rage `[-1, 1]` for each of the joints at both the hips and the knees."
      ],
      "metadata": {
        "id": "dKGIwjAcHBmh"
      },
      "id": "dKGIwjAcHBmh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Observation Space\n",
        "The agent's observation space is a state vector containing 24 variables:\n",
        "1. Hull angle $[0, 2\\pi]$\n",
        "2. Angular Velocity $Continuous$\n",
        "3. Horizontal Speed $[-1, 1]$\n",
        "4. Vertical Speed $[-1, 1]$\n",
        "5. Position of Joints (4 Joints) $Continuous$\n",
        "6. Joints Angular Speed (4 Joints) $Continuous$\n",
        "7. Legs Contact with Ground (2 legs) $Bool$\n",
        "8. Lidar Readings (10) $Continuous$\n"
      ],
      "metadata": {
        "id": "VGjFG3boHb4K"
      },
      "id": "VGjFG3boHb4K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Rewards\n",
        "- Moving forward: Up to 300+ Points till the far end\n",
        "- Falling: -100\n",
        "- Applying motor torque: Lose small amount of points.\n",
        "\n",
        "## 2.4 Episode Termination\n",
        "An episode ends whenever the walker falls or if it reaches the far right end of thee environment"
      ],
      "metadata": {
        "id": "Qg34xGn_IwBu"
      },
      "id": "Qg34xGn_IwBu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loading the Environement\n",
        "We use the `gym` library to open the `BipedalWalker-v3` environment. `.reset()` resets the environment and `.render()` renders the first frame of the environment."
      ],
      "metadata": {
        "id": "sejlakzRMp-K"
      },
      "id": "sejlakzRMp-K"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "env.reset()\n",
        "\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "id": "bH3Bw4upNOn5",
        "outputId": "89c18262-ad44-47ee-9e4a-cdada8a28107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "id": "bH3Bw4upNOn5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x400 at 0x7EFD72351EB0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thereafter, we check the size of the state vector as well as the number of possible actions "
      ],
      "metadata": {
        "id": "UjSfxNuPNfvn"
      },
      "id": "UjSfxNuPNfvn"
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape = env.observation_space.shape\n",
        "poss_actions = env.action_space.shape\n",
        "\n",
        "print(f\"State Vector shape: {state_shape}\")\n",
        "print(f\"Number of Possible Actions: {poss_actions}\")"
      ],
      "metadata": {
        "id": "eSPw4v_EO2jK",
        "outputId": "774e0f3d-c01a-4c36-ad0e-91b92140c296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eSPw4v_EO2jK",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Vector shape: (24,)\n",
            "Number of Possible Actions: (4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Dynamics of the Environment\n",
        "Running `.step(action)` on the environment runs a single time step from the current state, $s$, taking action $a$ and returning 4 different values:\n",
        "\n",
        "1. `next_state` $(object)$: Observation of the environment after an action. This is a vector contain our Observation Space variables. We may sometimes denote this as $s'$\n",
        "\n",
        "2. `rewards` $(float)$: Reward received as a result of our action. We may sometimes denote this as $R$\n",
        "\n",
        "3. `done` $(bool)$: Indiciating whether an episode has terminated\n",
        "\n",
        "4. `info` $(dictionary)$: Diagnostics used for debugging."
      ],
      "metadata": {
        "id": "E3gkbnmnQasz"
      },
      "id": "E3gkbnmnQasz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial state of environment\n",
        "init_state = env.reset()\n",
        "\n",
        "# Sample action\n",
        "action = [1, 1, 1, 1]\n",
        "\n",
        "# Run a single time step given the action\n",
        "next_state, reward, done, _ = env.step(action) "
      ],
      "metadata": {
        "id": "9fcdAgTcRpvv"
      },
      "id": "9fcdAgTcRpvv",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_table(init_state, action, next_state, reward, done)"
      ],
      "metadata": {
        "id": "Wzf8UwFSUXAu",
        "outputId": "9eb71396-87fc-4a03-dd3c-f38a1389ded2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "id": "Wzf8UwFSUXAu",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<class 'statsmodels.iolib.table.SimpleTable'>"
            ],
            "text/html": [
              "<table class=\"simpletable\">\n",
              "<tr>\n",
              "  <th>Initial State:</th>       <td>[0.003 0.000 -0.002 -0.016 0.093 0.005 0.860 -0.002 1.000 0.033 0.005\n",
              " 0.853 -0.003 1.000 0.441 0.446 0.461 0.490 0.534 0.602 0.709 0.886 1.000\n",
              " 1.000]</td> \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Action:</th>                                                                                         <td>1</td>                                                                            \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Next State:</th>         <td>[-0.018 -0.066 -0.017 0.028 -0.234 0.193 1.453 0.834 1.000 -0.338 -0.273\n",
              " 1.708 0.992 1.000 0.457 0.462 0.478 0.508 0.554 0.625 0.735 0.919 1.000\n",
              " 1.000]</td>\n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Reward Received:</th>                                                                             <td>-0.145</td>                                                                          \n",
              "</tr>\n",
              "<tr>\n",
              "  <th>Episode Terminated:</th>                                                                           <td>False</td>                                                                          \n",
              "</tr>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Deep $Q$-Learning\n",
        "## 5.1 $Q$ Function\n",
        "Our $Q$ function takes in 2 inputs -- $s$ and $a$, the current state and the action taken from this state. $Q(s, a)$ in this case is the total return for taking action $a$, arriving at state $s'$, and performing optimal actions from then on. We formulate $Q(s, a)$ using the Bellman Equation as shown:\n",
        "\n",
        "$$Q_{i+1}(s, a) = R + \\gamma\\ max_{a'}\\ Q_{i}(s', a')$$\n",
        "\n",
        "Whereby $\\gamma$ is the discount factor. As $i \\to \\infty$, our $Q$ function converges to the optimal $Q^{*}$. Due to the continuous problem that we are facing, we are unable to explore the entire state-action space and instead make use of a neural network, denoted as the $Q$-network, to estimate $Q(s, a) \\approx Q^{*}(s, a)$ via iteratively adjusting its weights using gradient descent.\n",
        "\n",
        "The $Q^{*}$ function is then used to choose the action that maximises $Q^{*}(s, a)$ to gain the greatest reward\n",
        "\n",
        "## 5.2 Target Network\n",
        "Our error term is currently calculated as shown\n",
        "$$\n",
        "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
        "$$\n",
        "\n",
        "However, the constantly changing $y$ targets may lead to instabilities in this application of reinforcement learning. Therefore, we need a seperate NN to be calculating the $y$ targets, whose weights are updated at a significantly slower pace. We denote the target network's state-action function as $\\hat{Q}$ and its weights as $w^{-}$. We update $w^{-}$  using a **soft update** as follows:\n",
        "\n",
        "$$w^-\\leftarrow \\tau w + (1 - \\tau) w^-$$\n",
        "\n",
        "whereby $\\tau < 1$. And our error is calculated as shown:\n",
        "\n",
        "$$ \\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}} $$\n",
        "\n",
        "## 5.3 Networks\n",
        "Here, the networks for both the target network as well as the $Q$-network are built using `keras`. Our initial architecture is as follows:\n",
        "1. `Input` layer: takes in `state_shape` as inpit\n",
        "2. `Dense` layer: `64` units, `relu` activation\n",
        "3. `Dense` layer: `64` units, `relu` activation\n",
        "4. `Dense` layer: `poss_actions` units, `linear` activation\n",
        "\n",
        "We use the `Adam` optimizer here, and also initate our intiial parameters."
      ],
      "metadata": {
        "id": "XwEMud4LUdT_"
      },
      "id": "XwEMud4LUdT_"
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMETERS\n",
        "ALPHA = 1e-3 # Learning rate\n",
        "GAMMA = 0.8 # Discount factor\n",
        "MEM_SIZE = 100000 # Memory buffer size\n",
        "NUM_STEPS_FOR_UPDATE = 4 # Number of time steps before updating weights"
      ],
      "metadata": {
        "id": "1_6BHxFJsF0c"
      },
      "id": "1_6BHxFJsF0c",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-network\n",
        "q_network = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(state_shape)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(poss_actions[0], activation='linear')\n",
        "])\n",
        "\n",
        "# Target Network\n",
        "target_network = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(state_shape)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(poss_actions[0], activation='linear')\n",
        "])\n",
        "\n",
        "# Optimiser\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=ALPHA)"
      ],
      "metadata": {
        "id": "0kuCCJ5xtsb2"
      },
      "id": "0kuCCJ5xtsb2",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Experience Relay\n",
        "**Experience Relay** is used to prevent consecutive time steps being used due to their higher correlation. This process consists of storing all our experiences, which are in the form $(s, a, R, s')$ in a **memory buffer**, which will be randomly sampled form in **mini-batches** that reduce runtime. We store each individual experienced as a `namedtuple`.\n",
        "\n",
        "Experience relay will be applied later on while training the agent."
      ],
      "metadata": {
        "id": "Wrl1lfWpx07g"
      },
      "id": "Wrl1lfWpx07g"
    },
    {
      "cell_type": "code",
      "source": [
        "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\" ])"
      ],
      "metadata": {
        "id": "nsJ81AQx3_6A"
      },
      "id": "nsJ81AQx3_6A",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Loss Function\n",
        "We compute the loss using Mean Squared Error, and the error term will follow section 5.1 whereby $y$-target is obtained from the Target Network. One thing to note is that $y$ no longer follows the Bellman equation if the next step is at a **terminal state**.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    y_j =\n",
        "    \\begin{cases}\n",
        "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
        "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
        "    \\end{cases}       \n",
        "\\end{equation}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "e8IEaCcW4Ite"
      },
      "id": "e8IEaCcW4Ite"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
        "    \"\"\" \n",
        "    Calculates the loss.\n",
        "    \n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
        "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
        "          \n",
        "    Returns:\n",
        "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
        "            the y targets and the Q(s,a) values.\n",
        "    \"\"\"\n",
        "    # Unpack mini-batch\n",
        "    states, actions, rewards, next_states, done_vals = experiences\n",
        "\n",
        "    # Find Max Q^(s, a) using the Target Network's Q^ function\n",
        "    max_qsa = tf.reduce_max(target_network(next_states), axis=-1)\n",
        "\n",
        "    # Compute y-target\n",
        "    y_targets = rewards + (1 - done_vals) * gamma * max_qsa\n",
        "\n",
        "    # Get Q(s,a) values from the Q-network\n",
        "    q_values = q_network(states)\n",
        "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
        "                                               tf.cast(actions, tf.int32)], axis=1))\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = tf.keras.losses.MSE(y_targets, q_values)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRo7vyj45Iao"
      },
      "id": "qRo7vyj45Iao",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Function to Update Network Weights\n",
        "A custom training loop is employed in order to update both the weights of the $Q$ and $\\hat{Q}$. A soft-update as mentioned in 5.2 is used for $\\hat{Q}$"
      ],
      "metadata": {
        "id": "Z6UITp4v6E1H"
      },
      "id": "Z6UITp4v6E1H"
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def agent_learn(experiences, gamma):\n",
        "    \"\"\"\n",
        "    Updates the weights of the Q networks.\n",
        "    \n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate the loss.\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(experiences, gamma, q_network, target_network)\n",
        "\n",
        "    # Get the gradients of the loss with respect to the weights.\n",
        "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "    \n",
        "    # Update the weights of the q_network.\n",
        "    optimiser.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "\n",
        "    # update the weights of target network via (soft update.\n",
        "    # Function from utils\n",
        "\n",
        "    TAU = 1e-3\n",
        "    update_target_network(q_network, target_network, TAU)"
      ],
      "metadata": {
        "id": "4bahrEU-6_Ci"
      },
      "id": "4bahrEU-6_Ci",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Agent Training"
      ],
      "metadata": {
        "id": "oZzBFTcn7D9n"
      },
      "id": "oZzBFTcn7D9n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep track of learning time\n",
        "start = time.time()\n",
        "\n",
        "# Time requirement for the env\n",
        "max_num_timesteps = 1600\n",
        "NUM_EPISODES = 2000 # Our rough estimate of the number of episodes we need\n",
        "\n",
        "# Requirement for how many trials to average\n",
        "num_p_av = 100 \n",
        "\n",
        "# Initial Epsilon for epsilon greedy policy\n",
        "epsilon = 1.0\n",
        "\n",
        "# Keeping track of point history\n",
        "total_point_history = []\n",
        "\n",
        "# Memory buffer, containing experience namedtuples\n",
        "memory_buffer = deque(maxlen=MEMORY_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHCMdveZi3qv",
        "outputId": "3e313063-2be5-458e-efa7-20ef2baa17ab"
      },
      "id": "aHCMdveZi3qv",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 701 bytes | 17.00 KiB/s, done.\n",
            "From https://github.com/bckhm/Bipedal-Walker-Training\n",
            "   901996f..0ac3f94  main       -> origin/main\n",
            "Updating 901996f..0ac3f94\n",
            "Fast-forward\n",
            " .ipynb_checkpoints/utils-checkpoint.py | 28 \u001b[32m+++++++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " utils.py                               | 28 \u001b[32m+++++++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 54 insertions(+), 2 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial target weights using q_network's weights\n",
        "target_network.set_weights(q_network.get_weights())\n",
        "\n",
        "# Iterate NUM_EPISODES times\n",
        "for i in range(NUM_EPISODES):\n",
        "  # Rest env and start from the initial state\n",
        "  state = env.reset()\n",
        "  total_points = 0\n",
        "\n",
        "  # Iterate max number of time steps\n",
        "  for t in range(max_num_timesteps):\n",
        "\n",
        "    # Choose action a using current state\n",
        "    state_qn = np.expand_dims(state, axis=0) # Reshape to fit the q network\n",
        "    q_values = q_network(state_qn) # Get Q(s, a) values given current state\n",
        "    action = get_action(q_values, epsilon) # In Utils, use epsilon greedy policy \n"
      ],
      "metadata": {
        "id": "9zM3z_lzj8xk"
      },
      "id": "9zM3z_lzj8xk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwNgRmstsyK7"
      },
      "id": "OwNgRmstsyK7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}