{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5",
      "metadata": {
        "id": "00659373-14c2-4d6a-b4d3-c0e6b4e6ced5"
      },
      "source": [
        "# Blackjack Agent using Deep-Q Learning\n",
        "In this notebook, we aim to train an agent to play the Blackjack via the use of reinforcement learning (Deep-Q Learning). We will treat the process of walking as a Markov Decision Process (MDP) in our learning efforts."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/Github/Atari-Assault-Agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC84zDpqZ_CO",
        "outputId": "8c60a88f-e9e6-437d-e8b3-16f6ec333ec2"
      },
      "id": "GC84zDpqZ_CO",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Github/Atari-Assault-Agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "401d8655-4b98-41b6-8043-239cf300d997",
      "metadata": {
        "id": "401d8655-4b98-41b6-8043-239cf300d997"
      },
      "source": [
        "# 1. Packages\n",
        "`Xvfb` and `gym[Box2D]` are also required in order to run our display and the environment respectively."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!apt-get install x11-utils > /dev/null 2>&1 \n",
        "!pip install pyglet > /dev/null 2>&1 \n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxfoOJl-adp8",
        "outputId": "ac910c4d-dc92-4928-e746-bd5d42ee46ad"
      },
      "id": "BxfoOJl-adp8",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHZN9WWMazj_",
        "outputId": "3305575f-a380-4264-ed12-f055c4a2e0b3"
      },
      "id": "pHZN9WWMazj_",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[box2d]) (2.2.0)\n",
            "Collecting box2d-py==2.3.5\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting swig==4.*\n",
            "  Downloading swig-4.1.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.11.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Failed to build box2d-py\n",
            "Installing collected packages: swig, box2d-py, pygame\n",
            "  Running setup.py install for box2d-py ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: box2d-py was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed box2d-py-2.3.5 pygame-2.1.0 swig-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65",
      "metadata": {
        "id": "1e7dd31d-5524-44f9-8a1c-3279e1af3e65"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "from utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d331466-0731-4931-9334-138a2110c485",
      "metadata": {
        "id": "3d331466-0731-4931-9334-138a2110c485"
      },
      "outputs": [],
      "source": [
        "# Virtual Display for rendering of the Bipedal Walker environment\n",
        "Display(visible=0, size=(840, 480)).start();\n",
        "\n",
        "# Random seed for tensorflow\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Assault Environment\n",
        "We will [OpenAI's Gym library](https://www.gymlibrary.dev/) to load and attempt to solve the Blackjack environment. \n",
        "\n",
        "The goal of the Blakcjack environment is to train an agent to beat the dealer in Blackjack by obtaining cards that sum close to 21, without going over 21, and yet still have a higher value thant the dealer's card.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<figure>\n",
        "  <img src =\"https://www.gymlibrary.dev/_images/blackjack.gif\" width = 40%>\n",
        "      <figcaption style = \"text-align: center; font-style: italic\">Atari Environment</figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "KQQxtBtSFAl1"
      },
      "id": "KQQxtBtSFAl1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Action Space\n",
        "The action space consists of two actions represented by discrete values.\n",
        "- `0`: Stick\n",
        "- `1`: Hit"
      ],
      "metadata": {
        "id": "dKGIwjAcHBmh"
      },
      "id": "dKGIwjAcHBmh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Observation Space\n",
        "The agent's observation space is a state vector containing 3 variables:\n",
        "1. Player's current sum\n",
        "2. Dealer's one showing card (1- 10)\n",
        "3. Whether a player holds a usable ace\n"
      ],
      "metadata": {
        "id": "VGjFG3boHb4K"
      },
      "id": "VGjFG3boHb4K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Rewards\n",
        "- Win game: +1\n",
        "- Lose game: -1\n",
        "- Draw: 0\n",
        "- Win game with natural Blackjack: +1.5 if `natural=True`, else +1"
      ],
      "metadata": {
        "id": "Qg34xGn_IwBu"
      },
      "id": "Qg34xGn_IwBu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Loading the Environement\n",
        "We use the `gym` library to open the `BipedalWalker-v3` environment. `.reset()` resets the environment and `.render()` renders the first frame of the environment."
      ],
      "metadata": {
        "id": "sejlakzRMp-K"
      },
      "id": "sejlakzRMp-K"
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v1', natural=True, sab=False)\n",
        "env.reset()\n",
        "\n",
        "PIL.Image.fromarray(env.render(mode='rgb_array'))"
      ],
      "metadata": {
        "id": "bH3Bw4upNOn5",
        "outputId": "163f970f-8661-4987-8a49-bb48c6b51b61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "id": "bH3Bw4upNOn5",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=600x500 at 0x7FB93935A3D0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thereafter, we check the size of the state vector as well as the number of possible actions "
      ],
      "metadata": {
        "id": "UjSfxNuPNfvn"
      },
      "id": "UjSfxNuPNfvn"
    },
    {
      "cell_type": "code",
      "source": [
        "state_shape = env.observation_space\n",
        "poss_actions = env.action_space\n",
        "\n",
        "print(f\"State Vector: {state_shape}\")\n",
        "print(f\"Number of Possible Actions: {poss_actions}\")"
      ],
      "metadata": {
        "id": "eSPw4v_EO2jK",
        "outputId": "668c732b-8144-4b12-9382-fb2d3864918b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "eSPw4v_EO2jK",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State Vector: Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
            "Number of Possible Actions: Discrete(2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Dynamics of the Environment\n",
        "Running `.step(action)` on the environment runs a single time step from the current state, $s$, taking action $a$ and returning 4 different values:\n",
        "\n",
        "1. `next_state` $(object)$: Observation of the environment after an action. This is a vector contain our Observation Space variables. We may sometimes denote this as $s'$\n",
        "\n",
        "2. `rewards` $(float)$: Reward received as a result of our action. We may sometimes denote this as $R$\n",
        "\n",
        "3. `done` $(bool)$: Indiciating whether an episode has terminated\n",
        "\n",
        "4. `info` $(dictionary)$: Diagnostics used for debugging."
      ],
      "metadata": {
        "id": "E3gkbnmnQasz"
      },
      "id": "E3gkbnmnQasz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial state of environment\n",
        "init_state = env.reset()\n",
        "\n",
        "# Sample action\n",
        "action = 0\n",
        "\n",
        "# Run a single time step given the action\n",
        "next_state, reward, done, _ = env.step(action) "
      ],
      "metadata": {
        "id": "9fcdAgTcRpvv"
      },
      "id": "9fcdAgTcRpvv",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_table(init_state, action, next_state, reward, done)"
      ],
      "metadata": {
        "id": "Wzf8UwFSUXAu",
        "outputId": "8d93891d-00b4-4faf-ddcd-dbdf33f5a7b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "id": "Wzf8UwFSUXAu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-37b97c1e400e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/Github/Atari-Assault-Agent/utils.py\u001b[0m in \u001b[0;36mdisplay_table\u001b[0;34m(initial_state, action, next_state, reward, done)\u001b[0m\n\u001b[1;32m     56\u001b[0m         table_info = [\n\u001b[1;32m     57\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial State:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"{initial_state}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"Hip1 Action:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"Knee1 Action:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m\"Hip2 Action:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Deep $Q$-Learning\n",
        "## 5.1 $Q$ Function\n",
        "Our $Q$ function takes in 2 inputs -- $s$ and $a$, the current state and the action taken from this state. $Q(s, a)$ in this case is the total return for taking action $a$, arriving at state $s'$, and performing optimal actions from then on. We formulate $Q(s, a)$ using the Bellman Equation as shown:\n",
        "\n",
        "$$Q_{i+1}(s, a) = R + \\gamma\\ max_{a'}\\ Q_{i}(s', a')$$\n",
        "\n",
        "Whereby $\\gamma$ is the discount factor. As $i \\to \\infty$, our $Q$ function converges to the optimal $Q^{*}$. Due to the continuous problem that we are facing, we are unable to explore the entire state-action space and instead make use of a neural network, denoted as the $Q$-network, to estimate $Q(s, a) \\approx Q^{*}(s, a)$ via iteratively adjusting its weights using gradient descent.\n",
        "\n",
        "The $Q^{*}$ function is then used to choose the action that maximises $Q^{*}(s, a)$ to gain the greatest reward\n",
        "\n",
        "## 5.2 Target Network\n",
        "Our error term is currently calculated as shown\n",
        "$$\n",
        "\\overbrace{\\underbrace{R + \\gamma \\max_{a'}Q(s',a'; w)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}}\n",
        "$$\n",
        "\n",
        "However, the constantly changing $y$ targets may lead to instabilities in this application of reinforcement learning. Therefore, we need a seperate NN to be calculating the $y$ targets, whose weights are updated at a significantly slower pace. We denote the target network's state-action function as $\\hat{Q}$ and its weights as $w^{-}$. We update $w^{-}$  using a **soft update** as follows:\n",
        "\n",
        "$$w^-\\leftarrow \\tau w + (1 - \\tau) w^-$$\n",
        "\n",
        "whereby $\\tau < 1$. And our error is calculated as shown:\n",
        "\n",
        "$$ \\overbrace{\\underbrace{R + \\gamma \\max_{a'}\\hat{Q}(s',a'; w^-)}_{\\rm {y~target}} - Q(s,a;w)}^{\\rm {Error}} $$\n",
        "\n",
        "## 5.3 Networks\n",
        "Here, the networks for both the target network as well as the $Q$-network are built using `keras`. Our initial architecture is as follows:\n",
        "1. `Input` layer: takes in `state_shape` as inpit\n",
        "2. `Dense` layer: `64` units, `relu` activation\n",
        "3. `Dense` layer: `64` units, `relu` activation\n",
        "4. `Dense` layer: `poss_actions` units, `linear` activation\n",
        "\n",
        "We use the `Adam` optimizer here, and also initate our intiial parameters."
      ],
      "metadata": {
        "id": "XwEMud4LUdT_"
      },
      "id": "XwEMud4LUdT_"
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMETERS\n",
        "ALPHA = 1e-3 # Learning rate\n",
        "GAMMA = 0.8 # Discount factor\n",
        "MEM_SIZE = 100000 # Memory buffer size\n",
        "NUM_STEPS_FOR_UPDATE = 4 # Number of time steps before updating weights"
      ],
      "metadata": {
        "id": "1_6BHxFJsF0c"
      },
      "id": "1_6BHxFJsF0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-network\n",
        "q_network = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(state_shape)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(poss_actions[0], activation='linear')\n",
        "])\n",
        "\n",
        "# Target Network\n",
        "target_network = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(state_shape)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(poss_actions[0], activation='linear')\n",
        "])\n",
        "\n",
        "# Optimiser\n",
        "optimiser = tf.keras.optimizers.Adam(learning_rate=ALPHA)"
      ],
      "metadata": {
        "id": "0kuCCJ5xtsb2"
      },
      "id": "0kuCCJ5xtsb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Experience Relay\n",
        "**Experience Relay** is used to prevent consecutive time steps being used due to their higher correlation. This process consists of storing all our experiences, which are in the form $(s, a, R, s')$ in a **memory buffer**, which will be randomly sampled form in **mini-batches** that reduce runtime. We store each individual experienced as a `namedtuple`.\n",
        "\n",
        "Experience relay will be applied later on while training the agent."
      ],
      "metadata": {
        "id": "Wrl1lfWpx07g"
      },
      "id": "Wrl1lfWpx07g"
    },
    {
      "cell_type": "code",
      "source": [
        "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\" ])"
      ],
      "metadata": {
        "id": "nsJ81AQx3_6A"
      },
      "id": "nsJ81AQx3_6A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.5 Loss Function\n",
        "We compute the loss using Mean Squared Error, and the error term will follow section 5.1 whereby $y$-target is obtained from the Target Network. One thing to note is that $y$ no longer follows the Bellman equation if the next step is at a **terminal state**.\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "    y_j =\n",
        "    \\begin{cases}\n",
        "      R_j & \\text{if episode terminates at step  } j+1\\\\\n",
        "      R_j + \\gamma \\max_{a'}\\hat{Q}(s_{j+1},a') & \\text{otherwise}\\\\\n",
        "    \\end{cases}       \n",
        "\\end{equation}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "e8IEaCcW4Ite"
      },
      "id": "e8IEaCcW4Ite"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
        "    \"\"\" \n",
        "    Calculates the loss.\n",
        "    \n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
        "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
        "          \n",
        "    Returns:\n",
        "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
        "            the y targets and the Q(s,a) values.\n",
        "    \"\"\"\n",
        "    # Unpack mini-batch\n",
        "    states, actions, rewards, next_states, done_vals = experiences\n",
        "\n",
        "    # Find Max Q^(s, a) using the Target Network's Q^ function\n",
        "    max_qsa = tf.reduce_max(target_network(next_states), axis=-1)\n",
        "\n",
        "    # Compute y-target\n",
        "    y_targets = rewards + (1 - done_vals) * gamma * max_qsa\n",
        "\n",
        "    # Get Q(s,a) values from the Q-network\n",
        "    q_values = q_network(states)\n",
        "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
        "                                               tf.cast(actions, tf.int32)], axis=1))\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = tf.keras.losses.MSE(y_targets, q_values)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qRo7vyj45Iao"
      },
      "id": "qRo7vyj45Iao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.6 Function to Update Network Weights\n",
        "A custom training loop is employed in order to update both the weights of the $Q$ and $\\hat{Q}$. A soft-update as mentioned in 5.2 is used for $\\hat{Q}$"
      ],
      "metadata": {
        "id": "Z6UITp4v6E1H"
      },
      "id": "Z6UITp4v6E1H"
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def agent_learn(experiences, gamma):\n",
        "    \"\"\"\n",
        "    Updates the weights of the Q networks.\n",
        "    \n",
        "    Args:\n",
        "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"done\"] namedtuples\n",
        "      gamma: (float) The discount factor.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate the loss.\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(experiences, gamma, q_network, target_network)\n",
        "\n",
        "    # Get the gradients of the loss with respect to the weights.\n",
        "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
        "    \n",
        "    # Update the weights of the q_network.\n",
        "    optimiser.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
        "\n",
        "    # update the weights of target network via (soft update.\n",
        "    # Function from utils\n",
        "\n",
        "    TAU = 1e-3\n",
        "    update_target_network(q_network, target_network, TAU)"
      ],
      "metadata": {
        "id": "4bahrEU-6_Ci"
      },
      "id": "4bahrEU-6_Ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Agent Training"
      ],
      "metadata": {
        "id": "oZzBFTcn7D9n"
      },
      "id": "oZzBFTcn7D9n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep track of learning time\n",
        "start = time.time()\n",
        "\n",
        "# Time requirement for the env\n",
        "max_num_timesteps = 1600\n",
        "NUM_EPISODES = 2000 # Our rough estimate of the number of episodes we need\n",
        "\n",
        "# Requirement for how many trials to average\n",
        "num_p_av = 100 \n",
        "\n",
        "# Initial Epsilon for epsilon greedy policy\n",
        "epsilon = 1.0\n",
        "\n",
        "# Keeping track of point history\n",
        "total_point_history = []\n",
        "\n",
        "# Memory buffer, containing experience namedtuples\n",
        "memory_buffer = deque(maxlen=MEMORY_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHCMdveZi3qv",
        "outputId": "3e313063-2be5-458e-efa7-20ef2baa17ab"
      },
      "id": "aHCMdveZi3qv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1)\u001b[K\rremote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects:  25% (1/4)\rUnpacking objects:  50% (2/4)\rUnpacking objects:  75% (3/4)\rUnpacking objects: 100% (4/4)\rUnpacking objects: 100% (4/4), 701 bytes | 17.00 KiB/s, done.\n",
            "From https://github.com/bckhm/Bipedal-Walker-Training\n",
            "   901996f..0ac3f94  main       -> origin/main\n",
            "Updating 901996f..0ac3f94\n",
            "Fast-forward\n",
            " .ipynb_checkpoints/utils-checkpoint.py | 28 \u001b[32m+++++++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " utils.py                               | 28 \u001b[32m+++++++++++++++++++++++++++\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 54 insertions(+), 2 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set initial target weights using q_network's weights\n",
        "target_network.set_weights(q_network.get_weights())\n",
        "\n",
        "# Iterate NUM_EPISODES times\n",
        "for i in range(NUM_EPISODES):\n",
        "  # Rest env and start from the initial state\n",
        "  state = env.reset()\n",
        "  total_points = 0\n",
        "\n",
        "  # Iterate max number of time steps\n",
        "  for t in range(max_num_timesteps):\n",
        "\n",
        "    # Choose action a using current state\n",
        "    state_qn = np.expand_dims(state, axis=0) # Reshape to fit the q network\n",
        "    q_values = q_network(state_qn) # Get Q(s, a) values given current state\n",
        "    action = get_action(q_values, epsilon) # In Utils, use epsilon greedy policy \n"
      ],
      "metadata": {
        "id": "9zM3z_lzj8xk"
      },
      "id": "9zM3z_lzj8xk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OwNgRmstsyK7"
      },
      "id": "OwNgRmstsyK7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}